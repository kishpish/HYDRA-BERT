# HYDRA-BERT Training Configuration
# Hydrogel Unified Deep Regression Architecture with BERT

model:
  polybert_path: "kuelumbus/polyBERT"
  num_property_features: 6
  num_patient_features: 4
  num_classes: 3
  freeze_polybert: true
  dropout: 0.1

training:
  epochs: 10
  batch_size: 128  # Per GPU
  gradient_accumulation_steps: 2
  learning_rate: 2.0e-5
  warmup_steps: 500
  weight_decay: 0.01
  max_grad_norm: 1.0
  seed: 42

  # Phase 2 fine-tuning (optional)
  phase2_enabled: false
  phase2_start_epoch: 11
  phase2_epochs: 5
  phase2_polybert_lr: 5.0e-6
  phase2_other_lr: 1.0e-4
  layer_lr_decay: 0.95

optimizer:
  type: "adamw"
  betas: [0.9, 0.999]
  eps: 1.0e-8

scheduler:
  type: "cosine"
  num_cycles: 0.5
  min_lr_ratio: 0.1

loss:
  type: "hydra"  # "hydra" for uncertainty-weighted, "simple" for fixed weights
  focal_gamma: 2.0
  regression_beta: 2.0
  aux_weight: 0.3
  class_weights: [1.0, 0.4, 2.0]  # harmful, beneficial, optimal

data:
  data_path: "data/processed/POLYBERT_TRAINING_FINAL.csv"
  train_split: 0.7
  val_split: 0.2
  test_split: 0.1
  max_seq_length: 128
  num_workers: 4
  pin_memory: true

  # Patient weighting
  real_patient_weight: 5.0
  use_patient_weighting: true

evaluation:
  eval_every_n_steps: 500
  save_every_n_epochs: 1
  early_stopping_patience: 5
  early_stopping_metric: "val_loss"

  # Metrics thresholds
  target_mae: 1.0
  target_r2: 0.80
  target_accuracy: 0.85
  target_f1: 0.75

logging:
  log_dir: "logs"
  checkpoint_dir: "checkpoints"
  results_dir: "results"
  log_every_n_steps: 50
  use_wandb: false
  wandb_project: "hydra-bert"
  wandb_entity: null

distributed:
  use_deepspeed: false  # Set to true only if CUDA toolkit is installed
  deepspeed_config: "configs/ds_config.json"
  fp16: true
  gradient_checkpointing: false
