# Accelerate configuration without DeepSpeed
# Uses PyTorch DDP for multi-GPU training
compute_environment: LOCAL_MACHINE
debug: false

distributed_type: MULTI_GPU
downcast_bf16: 'no'

machine_rank: 0
main_process_ip: null
main_process_port: null
main_training_function: main

mixed_precision: fp16
num_machines: 1
num_processes: 16  # 16x A100 GPUs

rdzv_backend: static
same_network: true
tpu_env: []
tpu_use_cluster: false
tpu_use_sudo: false
use_cpu: false
